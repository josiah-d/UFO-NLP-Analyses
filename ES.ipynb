{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24de85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159188cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting params\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "plt.rcParams['figure.titlesize'] = 24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177e9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Clean import CleanUFOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ff3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "cufo = CleanUFOs('data/ufodata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584c08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cufo.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019230b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occured</th>\n",
       "      <th>reported</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/6/2017 05:00</td>\n",
       "      <td>5/6/2017 4:10:01 AM 04:10</td>\n",
       "      <td>Camp McGregor</td>\n",
       "      <td>NM</td>\n",
       "      <td>Light</td>\n",
       "      <td>10 minutes</td>\n",
       "      <td>Light seen over mountain's east of Camp McGreg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/6/2017 04:50</td>\n",
       "      <td>5/6/2017 5:00:54 AM 05:00</td>\n",
       "      <td>Mojave (Canada)</td>\n",
       "      <td>BC</td>\n",
       "      <td>Light</td>\n",
       "      <td>1</td>\n",
       "      <td>Light in sky stationary.  Not a airplane or an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/5/2017 11:30</td>\n",
       "      <td>5/5/2017 12:18:44 PM 12:18</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>Disk</td>\n",
       "      <td>3 seconds</td>\n",
       "      <td>Flying saucer descends, possibly lands in Nort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/5/2017 03:00</td>\n",
       "      <td>5/5/2017 3:49:05 AM 03:49</td>\n",
       "      <td>El Mirage</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Circle</td>\n",
       "      <td>30 seconds</td>\n",
       "      <td>While letting my dog out, a very bright white ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/4/2017 23:34</td>\n",
       "      <td>5/4/2017 10:38:52 PM 22:38</td>\n",
       "      <td>York</td>\n",
       "      <td>NE</td>\n",
       "      <td>Fireball</td>\n",
       "      <td>0</td>\n",
       "      <td>A fire ball was moving in the atmosphere while...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          occured                    reported             city state  \\\n",
       "0  5/6/2017 05:00   5/6/2017 4:10:01 AM 04:10    Camp McGregor    NM   \n",
       "1  5/6/2017 04:50   5/6/2017 5:00:54 AM 05:00  Mojave (Canada)    BC   \n",
       "2  5/5/2017 11:30  5/5/2017 12:18:44 PM 12:18           Austin    TX   \n",
       "3  5/5/2017 03:00   5/5/2017 3:49:05 AM 03:49        El Mirage    AZ   \n",
       "4  5/4/2017 23:34  5/4/2017 10:38:52 PM 22:38             York    NE   \n",
       "\n",
       "      shape    duration                                        description  \n",
       "0     Light  10 minutes  Light seen over mountain's east of Camp McGreg...  \n",
       "1     Light           1  Light in sky stationary.  Not a airplane or an...  \n",
       "2      Disk   3 seconds  Flying saucer descends, possibly lands in Nort...  \n",
       "3    Circle  30 seconds  While letting my dog out, a very bright white ...  \n",
       "4  Fireball           0  A fire ball was moving in the atmosphere while...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4011e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = df['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1234605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# import string\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# def filter_tokens(sent):\n",
    "#         punctuation_ = set(string.punctuation)\n",
    "#         stopwords_ = set(stopwords.words('english'))\n",
    "#         return([w for w in sent if not w in stopwords_ and not w in punctuation_])\n",
    "\n",
    "# def tokenize(text):\n",
    "#     sent_tokens = sent_tokenize(text)\n",
    "#     tokens = [sent for sent in map(word_tokenize, sent_tokens)]\n",
    "\n",
    "#     tokens_lower = [[word.lower() for word in sent] for sent in tokens]\n",
    "\n",
    "   \n",
    "    \n",
    "#     tokens_filtered = list(map(filter_tokens, tokens_lower))\n",
    "\n",
    "#     flattened = []\n",
    "#     for item in tokens_filtered:\n",
    "#         if type(item) == list:\n",
    "#             for sub_item in item:\n",
    "#                 flattened.append(sub_item)\n",
    "#         else: flattenedned.append(item)\n",
    "\n",
    "#     for word in flattened:\n",
    "#         if \"''\" in flattened:\n",
    "#             flattened.remove(\"''\")\n",
    "#     for word in flattened:\n",
    "#         if \"``\" in flattened:\n",
    "#             flattened.remove(\"``\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f3bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4ad16f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141adfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/everett/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<98x1831 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6290 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfidf.fit_transform(df['description'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0128f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1776)\t0.1287032851010753\n",
      "  (0, 1769)\t0.15287530430271395\n",
      "  (0, 1755)\t0.06777430091426412\n",
      "  (0, 1736)\t0.22796045401633586\n",
      "  (0, 1554)\t0.19041787915952488\n",
      "  (0, 1532)\t0.17833186955870556\n",
      "  (0, 1447)\t0.12066448856741595\n",
      "  (0, 1415)\t0.12657899375171847\n",
      "  (0, 1349)\t0.1136660598966861\n",
      "  (0, 1206)\t0.10324671984508368\n",
      "  (0, 1120)\t0.10324671984508368\n",
      "  (0, 1117)\t0.09932965298367935\n",
      "  (0, 1063)\t0.2158744444155165\n",
      "  (0, 1014)\t0.2655030288731468\n",
      "  (0, 991)\t0.08800472129066028\n",
      "  (0, 975)\t0.09808448351162648\n",
      "  (0, 965)\t0.15061799241821025\n",
      "  (0, 850)\t0.1495917240378837\n",
      "  (0, 826)\t0.2059994555445845\n",
      "  (0, 819)\t0.15287530430271395\n",
      "  (0, 816)\t0.2158744444155165\n",
      "  (0, 613)\t0.13562705836843741\n",
      "  (0, 559)\t0.1287032851010753\n",
      "  (0, 458)\t0.22796045401633586\n",
      "  (0, 369)\t0.2655030288731468\n",
      "  :\t:\n",
      "  (97, 767)\t0.0875617327831301\n",
      "  (97, 719)\t0.06411643806729929\n",
      "  (97, 698)\t0.07761428442336672\n",
      "  (97, 685)\t0.12113208014199049\n",
      "  (97, 658)\t0.0875617327831301\n",
      "  (97, 627)\t0.07761428442336672\n",
      "  (97, 531)\t0.07106206769310949\n",
      "  (97, 509)\t0.06411643806729929\n",
      "  (97, 499)\t0.08195962038585784\n",
      "  (97, 450)\t0.09545746674192528\n",
      "  (97, 446)\t0.15522856884673344\n",
      "  (97, 395)\t0.09545746674192528\n",
      "  (97, 373)\t0.09545746674192528\n",
      "  (97, 312)\t0.09545746674192528\n",
      "  (97, 296)\t0.0875617327831301\n",
      "  (97, 295)\t0.08195962038585784\n",
      "  (97, 254)\t0.07761428442336672\n",
      "  (97, 227)\t0.10992785534744594\n",
      "  (97, 207)\t0.07406388642706267\n",
      "  (97, 138)\t0.035264743918169944\n",
      "  (97, 64)\t0.09545746674192528\n",
      "  (97, 34)\t0.07106206769310949\n",
      "  (97, 15)\t0.33780196104214716\n",
      "  (97, 12)\t0.0927582238670365\n",
      "  (97, 11)\t0.0927582238670365\n"
     ]
    }
   ],
   "source": [
    "response = tfidf.transform(lst)\n",
    "print( response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0f3c429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'d\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " \"'wing\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '--',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '.....',\n",
       " '......',\n",
       " '........',\n",
       " '.........',\n",
       " '.after',\n",
       " '.it',\n",
       " '.so',\n",
       " '01:45',\n",
       " '03-05-17',\n",
       " '05:45',\n",
       " '1',\n",
       " '1,000',\n",
       " '1/10th',\n",
       " '10',\n",
       " '10,000',\n",
       " '100',\n",
       " '1000',\n",
       " '10:00-11:00',\n",
       " '10:30',\n",
       " '11',\n",
       " '1100',\n",
       " '119',\n",
       " '11:15',\n",
       " '11:30',\n",
       " '11am',\n",
       " '12',\n",
       " '12-14',\n",
       " '12:13',\n",
       " '12:30',\n",
       " '12:30am.i',\n",
       " '13',\n",
       " '14',\n",
       " '14-18',\n",
       " '15',\n",
       " '15-20',\n",
       " '150m',\n",
       " '1970',\n",
       " '1:00am',\n",
       " '2',\n",
       " '2-10',\n",
       " '20',\n",
       " '20-25',\n",
       " '20-30',\n",
       " '200',\n",
       " '200ft',\n",
       " '2012',\n",
       " '2013-2014',\n",
       " '2015.',\n",
       " '2016',\n",
       " '2017',\n",
       " '2017.',\n",
       " '2017.first',\n",
       " '23:00',\n",
       " '25',\n",
       " '27',\n",
       " '27th',\n",
       " '28',\n",
       " '28th',\n",
       " '29th',\n",
       " '2:00',\n",
       " '2:12',\n",
       " '2nd',\n",
       " '2pm',\n",
       " '3',\n",
       " '3-4',\n",
       " '30',\n",
       " '306',\n",
       " '331',\n",
       " '35',\n",
       " '35-40',\n",
       " '36',\n",
       " '4',\n",
       " '4/20/2017',\n",
       " '4/29/2017',\n",
       " '40',\n",
       " '40-50',\n",
       " '41',\n",
       " '43',\n",
       " '45',\n",
       " '45.',\n",
       " '4:00',\n",
       " '4:50',\n",
       " '4th',\n",
       " '5',\n",
       " '5-6',\n",
       " '50',\n",
       " '50-100',\n",
       " '500',\n",
       " '500-1000',\n",
       " '50th',\n",
       " '50x',\n",
       " '51',\n",
       " '575',\n",
       " '5th',\n",
       " '6',\n",
       " '6-8',\n",
       " '60',\n",
       " '6000',\n",
       " '60°',\n",
       " '63',\n",
       " '7',\n",
       " '70',\n",
       " '700-1000',\n",
       " '75',\n",
       " '75-80',\n",
       " '7:05',\n",
       " '7:30',\n",
       " '8',\n",
       " '80',\n",
       " '87',\n",
       " '8:07.',\n",
       " '8:25pm',\n",
       " '8pm',\n",
       " '9',\n",
       " '90',\n",
       " '94/29/2017',\n",
       " '99',\n",
       " '9:00',\n",
       " '9:30',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " '`',\n",
       " '``',\n",
       " 'a.m',\n",
       " 'abl',\n",
       " 'abov',\n",
       " 'abruptli',\n",
       " 'absolut',\n",
       " 'accident.i',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'acknowledg',\n",
       " 'acr',\n",
       " 'act',\n",
       " 'activ',\n",
       " 'activated.i',\n",
       " 'actual',\n",
       " 'addendum',\n",
       " 'address',\n",
       " 'adjac',\n",
       " 'adrenalin',\n",
       " 'advertis',\n",
       " 'aerial',\n",
       " 'aerodynam',\n",
       " 'afbwhil',\n",
       " 'afraid',\n",
       " 'afternoon',\n",
       " 'afterwards.a',\n",
       " 'againmi',\n",
       " 'agil',\n",
       " 'agl',\n",
       " 'ago',\n",
       " 'agre',\n",
       " 'ahead',\n",
       " 'air',\n",
       " 'air-assault',\n",
       " 'aircraft',\n",
       " 'airforc',\n",
       " 'airlin',\n",
       " 'airplan',\n",
       " 'airplanew',\n",
       " 'airport',\n",
       " 'alabama',\n",
       " 'alarm',\n",
       " 'albani',\n",
       " 'albuquerqu',\n",
       " 'alien',\n",
       " 'alleg',\n",
       " 'allow',\n",
       " 'alon',\n",
       " 'alot',\n",
       " 'alreadi',\n",
       " 'altitud',\n",
       " 'altitudei',\n",
       " 'aluminum',\n",
       " 'alway',\n",
       " 'amaz',\n",
       " 'amber',\n",
       " 'amber/whit',\n",
       " 'amend',\n",
       " 'amp',\n",
       " 'anchorag',\n",
       " 'and/but',\n",
       " 'andi',\n",
       " 'andrew',\n",
       " 'angel',\n",
       " 'angl',\n",
       " 'ani',\n",
       " 'anniston',\n",
       " 'anonym',\n",
       " 'anoth',\n",
       " 'answer',\n",
       " 'antiqu',\n",
       " 'anxieti',\n",
       " 'any.i',\n",
       " 'anybodi',\n",
       " 'anymor',\n",
       " 'anyon',\n",
       " 'anyth',\n",
       " 'anywher',\n",
       " 'apart',\n",
       " 'appar',\n",
       " 'appear',\n",
       " 'appendag',\n",
       " 'approach',\n",
       " 'approx',\n",
       " 'approxim',\n",
       " 'approximatley',\n",
       " 'april',\n",
       " 'aprox',\n",
       " 'arang',\n",
       " 'arc',\n",
       " 'arch',\n",
       " 'area',\n",
       " 'arizona',\n",
       " 'arm',\n",
       " 'armi',\n",
       " 'around.seen',\n",
       " 'arrang',\n",
       " 'array',\n",
       " 'array.th',\n",
       " 'arrow',\n",
       " 'asap',\n",
       " 'ascertain',\n",
       " 'ask',\n",
       " 'asleep',\n",
       " 'aspirin',\n",
       " 'assum',\n",
       " 'asteroid',\n",
       " 'aswel',\n",
       " 'ate',\n",
       " 'atmospher',\n",
       " 'atmosphere.a',\n",
       " 'attach',\n",
       " 'attend',\n",
       " 'attent',\n",
       " 'attention.12:09',\n",
       " 'austin',\n",
       " 'avenu',\n",
       " 'await',\n",
       " 'awaken',\n",
       " 'away',\n",
       " 'az',\n",
       " 'b.',\n",
       " 'b4',\n",
       " 'backdoor',\n",
       " 'backyard',\n",
       " 'baker',\n",
       " 'balconi',\n",
       " 'ball',\n",
       " 'ball/cigar',\n",
       " 'balloon',\n",
       " 'bar.thes',\n",
       " 'barona',\n",
       " 'base',\n",
       " 'basketbal',\n",
       " 'bc',\n",
       " 'beacon',\n",
       " 'beam',\n",
       " 'beauti',\n",
       " 'becam',\n",
       " 'becaus',\n",
       " 'becker',\n",
       " 'becom',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'befo',\n",
       " 'befor',\n",
       " 'before.i',\n",
       " 'began',\n",
       " 'believ',\n",
       " 'bell',\n",
       " 'bell/sauc',\n",
       " 'bellow',\n",
       " 'belt',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'bigger.it',\n",
       " 'binocular',\n",
       " 'biologist',\n",
       " 'bird',\n",
       " 'bit',\n",
       " 'bizarr',\n",
       " 'black',\n",
       " 'blackhawk',\n",
       " 'blade',\n",
       " 'blend',\n",
       " 'blew',\n",
       " 'blink',\n",
       " 'block',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'blue-green',\n",
       " 'blue-violet',\n",
       " 'blueish',\n",
       " 'bluff',\n",
       " 'bluish',\n",
       " 'blurri',\n",
       " 'bob',\n",
       " 'bodi',\n",
       " 'boe',\n",
       " 'bolt',\n",
       " 'bonn',\n",
       " 'boom',\n",
       " 'boon',\n",
       " 'bound',\n",
       " 'box',\n",
       " 'boyfriend',\n",
       " 'break',\n",
       " 'break-in',\n",
       " 'brentwood',\n",
       " 'brick',\n",
       " 'briefli',\n",
       " 'bright',\n",
       " 'brighter',\n",
       " 'brightest',\n",
       " 'brillianc',\n",
       " 'brilliant',\n",
       " 'british',\n",
       " 'broad',\n",
       " 'broadway',\n",
       " 'brockvil',\n",
       " 'broke',\n",
       " 'brooklyn',\n",
       " 'brooklyn/nyc',\n",
       " 'brother',\n",
       " 'brown',\n",
       " 'brush',\n",
       " 'bu',\n",
       " 'buffet',\n",
       " 'bug',\n",
       " 'build',\n",
       " 'building-s',\n",
       " 'bulb',\n",
       " 'bulki',\n",
       " 'bump',\n",
       " 'bunch',\n",
       " 'burbank',\n",
       " 'busi',\n",
       " 'button',\n",
       " 'ca',\n",
       " 'caesar',\n",
       " 'california',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'campground',\n",
       " 'canada',\n",
       " 'canada.though',\n",
       " 'candi',\n",
       " 'candl',\n",
       " 'capsul',\n",
       " 'captur',\n",
       " 'car',\n",
       " 'caravan',\n",
       " 'cari',\n",
       " 'cart',\n",
       " 'casino',\n",
       " 'cast',\n",
       " 'cat',\n",
       " 'caught',\n",
       " 'caus',\n",
       " 'cause.i',\n",
       " 'cell',\n",
       " 'cellphon',\n",
       " 'cent',\n",
       " 'center',\n",
       " 'central',\n",
       " 'ch-47',\n",
       " 'chambersburg',\n",
       " 'chang',\n",
       " 'channel',\n",
       " 'chappel',\n",
       " 'characterist',\n",
       " 'chase',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'chines',\n",
       " 'chinook',\n",
       " 'christian',\n",
       " 'cigar',\n",
       " 'cigarett',\n",
       " 'circada',\n",
       " 'circl',\n",
       " 'circlei',\n",
       " 'circular',\n",
       " 'cirlc',\n",
       " 'citi',\n",
       " 'claim',\n",
       " 'class',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearli',\n",
       " 'clearly.it',\n",
       " 'clip',\n",
       " 'clockwis',\n",
       " 'close',\n",
       " 'closer',\n",
       " 'closest',\n",
       " 'cloth',\n",
       " 'cloud',\n",
       " 'cloud-to-cloud',\n",
       " 'cloudi',\n",
       " 'clue',\n",
       " 'colleagu',\n",
       " 'collid',\n",
       " 'colon',\n",
       " 'color',\n",
       " 'colorblind',\n",
       " 'colorit',\n",
       " 'colour',\n",
       " 'columbia',\n",
       " 'combat',\n",
       " 'combin',\n",
       " 'come',\n",
       " 'comet',\n",
       " 'comment',\n",
       " 'commerci',\n",
       " 'common',\n",
       " 'commun',\n",
       " 'compact',\n",
       " 'compani',\n",
       " 'compar',\n",
       " 'comparison',\n",
       " 'compet',\n",
       " 'complet',\n",
       " 'comprehens',\n",
       " 'compris',\n",
       " 'concentr',\n",
       " 'concern',\n",
       " 'cone',\n",
       " 'confer',\n",
       " 'confirm',\n",
       " 'connect',\n",
       " 'consist',\n",
       " 'contact',\n",
       " 'contin',\n",
       " 'continu',\n",
       " 'contrail',\n",
       " 'control',\n",
       " 'convent',\n",
       " 'convinc',\n",
       " 'cook',\n",
       " 'corkscrew',\n",
       " 'corkscrewslook',\n",
       " 'corner',\n",
       " 'cornfield',\n",
       " 'correct',\n",
       " 'cought',\n",
       " 'could.th',\n",
       " 'couldn',\n",
       " 'council',\n",
       " 'counter',\n",
       " 'countri',\n",
       " 'coupl',\n",
       " 'cours',\n",
       " 'cover',\n",
       " 'coyot',\n",
       " 'crack',\n",
       " 'craft',\n",
       " 'crash',\n",
       " 'crazi',\n",
       " 'craziest',\n",
       " 'cream',\n",
       " 'creek',\n",
       " 'crescent',\n",
       " 'crest',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'cruis',\n",
       " 'cruze',\n",
       " 'crystal',\n",
       " 'cst',\n",
       " 'cst.i',\n",
       " 'curios',\n",
       " 'current',\n",
       " 'curv',\n",
       " 'dad',\n",
       " 'daddi',\n",
       " 'damond',\n",
       " 'danc',\n",
       " 'dancing/fli',\n",
       " 'dark',\n",
       " 'darker',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'dave',\n",
       " 'davenport',\n",
       " 'day',\n",
       " 'day.when',\n",
       " 'daylight',\n",
       " 'deal',\n",
       " 'decend',\n",
       " 'decid',\n",
       " 'deck',\n",
       " 'defeat',\n",
       " 'defi',\n",
       " 'defin',\n",
       " 'definit',\n",
       " 'degre',\n",
       " 'delet',\n",
       " 'deliber',\n",
       " 'demott',\n",
       " 'depend',\n",
       " 'descend',\n",
       " 'descent.i',\n",
       " 'describ',\n",
       " 'descript',\n",
       " 'desert',\n",
       " 'desol',\n",
       " 'deton',\n",
       " 'detroit',\n",
       " 'develop',\n",
       " 'development',\n",
       " 'diagon',\n",
       " 'diamet',\n",
       " 'diamond',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'differ',\n",
       " 'differenti',\n",
       " 'difficult',\n",
       " 'dim',\n",
       " 'diminish',\n",
       " 'dimmer',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'dipper',\n",
       " 'direct',\n",
       " 'direction.i',\n",
       " 'directions.11:17',\n",
       " 'directions.3',\n",
       " 'directli',\n",
       " 'dirti',\n",
       " 'disappear',\n",
       " 'disappearing.a',\n",
       " 'discern',\n",
       " 'disk',\n",
       " 'dismiss',\n",
       " 'dispers',\n",
       " 'display',\n",
       " 'dissapear',\n",
       " 'dissip',\n",
       " 'distac',\n",
       " 'distanc',\n",
       " 'distinct',\n",
       " 'dive',\n",
       " 'divin',\n",
       " 'dix',\n",
       " 'doc.a',\n",
       " 'dock',\n",
       " 'doctor',\n",
       " 'dodg',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'dog',\n",
       " 'dollar',\n",
       " 'don',\n",
       " 'dongl',\n",
       " 'door',\n",
       " 'dot',\n",
       " 'doubt',\n",
       " 'downtown',\n",
       " 'downward',\n",
       " 'dozen',\n",
       " 'draw',\n",
       " 'dri',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'drive-up',\n",
       " 'driven',\n",
       " 'driver',\n",
       " 'driveway',\n",
       " 'drone',\n",
       " 'drop',\n",
       " 'drove',\n",
       " 'drum',\n",
       " 'duck',\n",
       " 'durat',\n",
       " 'dure',\n",
       " 'dusk',\n",
       " 'duti',\n",
       " 'ear',\n",
       " 'earli',\n",
       " 'earlier',\n",
       " 'earth',\n",
       " 'easi',\n",
       " 'easili',\n",
       " 'east',\n",
       " 'east/south',\n",
       " 'eastern',\n",
       " 'eastward',\n",
       " 'ect',\n",
       " 'eden',\n",
       " 'edg',\n",
       " 'edges.i',\n",
       " 'edit',\n",
       " 'edmonton',\n",
       " 'edt',\n",
       " 'effortlessli',\n",
       " 'elder',\n",
       " 'elect',\n",
       " 'electr',\n",
       " 'elev',\n",
       " 'els',\n",
       " 'elsewher',\n",
       " 'email',\n",
       " 'emit',\n",
       " 'emot',\n",
       " 'encount',\n",
       " 'encourag',\n",
       " 'end',\n",
       " 'engin',\n",
       " 'enjoy',\n",
       " 'enter',\n",
       " 'entir',\n",
       " 'equip',\n",
       " 'er',\n",
       " 'errat',\n",
       " 'especi',\n",
       " 'essenti',\n",
       " 'estim',\n",
       " 'event',\n",
       " 'event.bi',\n",
       " 'eventu',\n",
       " 'everi',\n",
       " 'everyday',\n",
       " 'everyon',\n",
       " 'everyth',\n",
       " 'evid',\n",
       " 'evil',\n",
       " 'exact',\n",
       " 'exactli',\n",
       " 'examin',\n",
       " 'excit',\n",
       " 'exclaim',\n",
       " 'exercis',\n",
       " 'exhaust',\n",
       " 'exit',\n",
       " 'experi',\n",
       " 'experienc',\n",
       " 'explain',\n",
       " 'explod',\n",
       " 'extra-terrestri',\n",
       " 'extrem',\n",
       " 'eye',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fade',\n",
       " 'fail',\n",
       " 'faint',\n",
       " 'fair',\n",
       " 'fairli',\n",
       " 'fall',\n",
       " 'famili',\n",
       " 'far',\n",
       " 'farm',\n",
       " 'farther',\n",
       " 'fashion',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'favorit',\n",
       " 'fear',\n",
       " 'featur',\n",
       " 'feel',\n",
       " 'feet',\n",
       " 'felt',\n",
       " 'femm',\n",
       " 'ferri',\n",
       " 'fiance.mi',\n",
       " 'fiancé',\n",
       " 'fickler',\n",
       " 'field',\n",
       " 'fifti',\n",
       " 'fighter',\n",
       " 'figur',\n",
       " 'final',\n",
       " 'finger',\n",
       " 'finish',\n",
       " 'firebal',\n",
       " 'fl',\n",
       " 'flame',\n",
       " 'flare',\n",
       " 'flash',\n",
       " 'flashingw',\n",
       " 'flashlight',\n",
       " 'flat',\n",
       " 'flaw',\n",
       " 'fleet',\n",
       " 'flew',\n",
       " 'fli',\n",
       " 'flicker',\n",
       " 'flight',\n",
       " 'float',\n",
       " 'flood',\n",
       " 'floodlight',\n",
       " 'flout',\n",
       " 'fluffi',\n",
       " 'flutter',\n",
       " 'flying-v',\n",
       " 'focu',\n",
       " 'fog',\n",
       " 'follow',\n",
       " 'food',\n",
       " 'footag',\n",
       " 'forc',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'form',\n",
       " 'format',\n",
       " 'fort',\n",
       " 'forth',\n",
       " 'forward',\n",
       " 'fraction',\n",
       " 'frame',\n",
       " 'freak',\n",
       " 'freeway',\n",
       " 'fridg',\n",
       " 'friend',\n",
       " 'frog',\n",
       " 'frozen',\n",
       " 'ft',\n",
       " 'ft.',\n",
       " 'fulli',\n",
       " 'furthest',\n",
       " 'fuselag',\n",
       " 'fuzzi',\n",
       " 'ga.mi',\n",
       " 'game',\n",
       " 'garag',\n",
       " 'gave',\n",
       " 'gaze',\n",
       " 'gear',\n",
       " 'gener',\n",
       " 'geniu',\n",
       " 'germantown',\n",
       " 'ghostish',\n",
       " 'giant',\n",
       " 'girlfriend',\n",
       " 'given',\n",
       " 'glanc',\n",
       " 'glass',\n",
       " 'glimps',\n",
       " 'glow',\n",
       " 'god',\n",
       " 'golf',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'goodby',\n",
       " 'googl',\n",
       " 'goos',\n",
       " 'got',\n",
       " 'govern',\n",
       " 'grab',\n",
       " 'grace',\n",
       " 'grade',\n",
       " 'gradual',\n",
       " 'gravity.th',\n",
       " 'gray',\n",
       " 'great',\n",
       " 'green',\n",
       " 'green/whit',\n",
       " 'greenish',\n",
       " 'grew',\n",
       " 'grey',\n",
       " 'grind',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'guess',\n",
       " 'ha',\n",
       " 'hade',\n",
       " 'haha',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handl',\n",
       " 'hang',\n",
       " 'happen',\n",
       " 'hard',\n",
       " 'hat-shap',\n",
       " 'hawaii',\n",
       " 'hazi',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heard.i',\n",
       " 'hearsay',\n",
       " 'heartbroken',\n",
       " 'heed',\n",
       " 'height',\n",
       " 'held',\n",
       " 'heli',\n",
       " 'helicopt',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'hidden',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highest',\n",
       " 'highland',\n",
       " 'highway',\n",
       " 'hill',\n",
       " 'hills.i',\n",
       " 'hinkson',\n",
       " 'histori',\n",
       " 'hit',\n",
       " 'hold',\n",
       " 'hole',\n",
       " 'holi',\n",
       " 'hollywood',\n",
       " 'home',\n",
       " 'honest',\n",
       " 'honestli',\n",
       " 'hope',\n",
       " 'horizon',\n",
       " 'horizont',\n",
       " 'hospit',\n",
       " 'hotline.11:50',\n",
       " 'hour',\n",
       " 'hous',\n",
       " 'houston',\n",
       " 'hover',\n",
       " 'howev',\n",
       " 'hue',\n",
       " 'huge',\n",
       " 'hum',\n",
       " 'humid',\n",
       " 'hundr',\n",
       " 'hurt',\n",
       " 'husband',\n",
       " 'hwi',\n",
       " 'hwy,10',\n",
       " 'i-287',\n",
       " 'i-45',\n",
       " 'i-5',\n",
       " 'i-95',\n",
       " 'i-b',\n",
       " 'ice',\n",
       " 'idea',\n",
       " 'identifi',\n",
       " 'ill',\n",
       " 'illumin',\n",
       " 'imiditley',\n",
       " 'immedi',\n",
       " 'immens',\n",
       " 'importantli',\n",
       " 'inch',\n",
       " 'increas',\n",
       " 'incred',\n",
       " 'inde',\n",
       " 'index',\n",
       " 'indian',\n",
       " 'indiana',\n",
       " 'indic',\n",
       " 'individu',\n",
       " 'industri',\n",
       " 'inform',\n",
       " 'insid',\n",
       " 'instantan',\n",
       " 'instantli',\n",
       " 'instead',\n",
       " 'instinct',\n",
       " 'institut',\n",
       " 'intens',\n",
       " 'intensifi',\n",
       " 'interestingli',\n",
       " 'intern',\n",
       " 'interrupt',\n",
       " 'investig',\n",
       " 'iphon',\n",
       " 'iphone.i',\n",
       " 'irrad',\n",
       " 'irredes',\n",
       " 'island',\n",
       " 'isn',\n",
       " 'it.i',\n",
       " 'it.it',\n",
       " 'item',\n",
       " 'jersey',\n",
       " 'jersey.on',\n",
       " 'jet',\n",
       " 'jet.i',\n",
       " 'jet.on',\n",
       " 'job',\n",
       " 'join',\n",
       " 'joint',\n",
       " 'judg',\n",
       " 'jump',\n",
       " 'just',\n",
       " 'kci',\n",
       " 'kelowna',\n",
       " 'kept',\n",
       " 'keyboard',\n",
       " 'kid',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kitchen',\n",
       " 'kite',\n",
       " 'kiteson',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'kt',\n",
       " 'l',\n",
       " 'laboratori',\n",
       " 'laboratory.w',\n",
       " 'ladi',\n",
       " 'lake',\n",
       " 'lamar.th',\n",
       " 'land',\n",
       " 'landing.a',\n",
       " 'landscap',\n",
       " 'lane',\n",
       " 'lantern',\n",
       " 'lantern…',\n",
       " 'lapd',\n",
       " 'lapin',\n",
       " 'laptop',\n",
       " 'larg',\n",
       " 'larger',\n",
       " 'laser',\n",
       " 'late',\n",
       " 'later',\n",
       " 'laundromat',\n",
       " 'lawrenc',\n",
       " 'layer',\n",
       " 'lean',\n",
       " 'leav',\n",
       " 'left',\n",
       " 'length',\n",
       " 'let',\n",
       " 'level',\n",
       " 'lie',\n",
       " 'life',\n",
       " 'lifetim',\n",
       " 'lift',\n",
       " 'light',\n",
       " 'light.i',\n",
       " 'lightbulb',\n",
       " 'lighter',\n",
       " 'lightlik',\n",
       " 'lightn',\n",
       " 'lights.dat',\n",
       " 'lights.ther',\n",
       " 'lightthi',\n",
       " 'lightwhil',\n",
       " 'like',\n",
       " 'line',\n",
       " 'line.then',\n",
       " 'line.thi',\n",
       " 'lit',\n",
       " 'liter',\n",
       " 'littl',\n",
       " 'live',\n",
       " 'load',\n",
       " 'local',\n",
       " 'locat',\n",
       " 'lock',\n",
       " 'logic',\n",
       " 'lone',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'look.bi',\n",
       " 'lookout',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'loud',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'lowest',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21226d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
